{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce0bd7e-ef5d-476f-a81a-35ebdd3a4d6f",
   "metadata": {},
   "source": [
    "XGBoost trained on two sites where GPP only good quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62577350-5a74-474b-9d7d-88355d179286",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import netCDF4\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaa38583-688b-4672-b73f-3c14f6c5cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define path to data\n",
    "path = '/cluster/home/akmete/MSc/Data/'\n",
    "files = [f for f in os.listdir(path) if f.endswith('.nc')] # all files\n",
    "files.sort()\n",
    "\n",
    "#Get the files out of folder which have intermediate NaN's. These are: DE-Lnf, IT-Ro2, SD-Dem, US-Los, US-Syv, US-WCr, US-Wi3 and ZM-Mon\n",
    "filt_list = ['DE-Lnf_flux.nc', 'DE-Lnf_meteo.nc', 'DE-Lnf_rs.nc', 'IT-Ro2_flux.nc', 'IT-Ro2_meteo.nc', 'IT-Ro2_rs.nc', 'SD-Dem_flux.nc', 'SD-Dem_meteo.nc', 'SD-Dem_rs.nc',\n",
    "'US-Los_flux.nc', 'US-Los_meteo.nc', 'US-Los_rs.nc', 'US-Syv_flux.nc', 'US-Syv_meteo.nc', 'US-Syv_rs.nc', 'US-WCr_flux.nc', 'US-WCr_meteo.nc', 'US-WCr_rs.nc',\n",
    "        'US-Wi3_flux.nc', 'US-Wi3_meteo.nc', 'US-Wi3_rs.nc', 'ZM-Mon_flux.nc', 'ZM-Mon_meteo.nc', 'ZM-Mon_rs.nc']\n",
    "\n",
    "# Remove files that are in list\n",
    "filtered_files = [file for file in files if file not in filt_list]\n",
    "assert len(filtered_files) % 3 == 0\n",
    "filtered_files = {filtered_files[0+3*i][:6]: (filtered_files[0+3*i],filtered_files[1+3*i],filtered_files[2+3*i]) for i in range(len(filtered_files)//3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e86751ec-9b61-4115-9a6c-4f25a8276530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_good_quality(df):\n",
    "    for col in df.columns:\n",
    "        # Skip quality control columns\n",
    "        if col.endswith('_qc'):\n",
    "            continue\n",
    "        # Construct the corresponding quality control column name\n",
    "        qc_col = f\"{col}_qc\"\n",
    "        if qc_col in df.columns:\n",
    "            # Create a mask for bad quality data\n",
    "            bad_quality_mask = df[qc_col].isin([2, 3])\n",
    "            # Set bad quality data points to NaN\n",
    "            df.loc[bad_quality_mask, col] = np.nan\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b09562-049e-4f83-883a-1d7e4f798f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "916a6754-d217-45fd-af1e-e33d71f5fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dataframe(file1, file2, file3):\n",
    "    #Open data\n",
    "    ds = xr.open_dataset(path+file1, engine='netcdf4')\n",
    "    dr = xr.open_dataset(path+file2, engine='netcdf4')\n",
    "    dt = xr.open_dataset(path+file3, engine='netcdf4')\n",
    "\n",
    "    #Convert to dataframe\n",
    "    df = ds.to_dataframe()\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    df_meteo = dr.to_dataframe()\n",
    "    df_meteo = pd.DataFrame(df_meteo)\n",
    "\n",
    "    df_rs = dt.to_dataframe()\n",
    "    df_rs = pd.DataFrame(df_rs)\n",
    "\n",
    "    #Get rid of 'x' and 'y' in multiindex and get rid of 'latitude' and 'longitude' in meteo file\n",
    "    df = df.droplevel(['x','y'])\n",
    "    df_meteo = df_meteo.droplevel(['x','y'])\n",
    "    df_rs = df_rs.droplevel(['x','y'])\n",
    "    df = df.drop('latitude', axis=1)\n",
    "    df = df.drop('longitude', axis=1)\n",
    "    df_meteo = df_meteo.drop('latitude', axis=1)\n",
    "    df_meteo = df_meteo.drop('longitude', axis=1)\n",
    "\n",
    "    #Truncate rs so that same amount of rows as flux and meteo\n",
    "    df_rs = df_rs.truncate(after='2021-12-31 23:45:00')\n",
    "\n",
    "    #Merge dataframes\n",
    "    df_combined = pd.concat([df, df_meteo, df_rs],axis=1)\n",
    "\n",
    "    #Mark bad quality data (for all variables which allow for that)\n",
    "    df_combined = extract_good_quality(df_combined)\n",
    "\n",
    "    #Add lagged variables and rolling mean\n",
    "    df_combined['lag_1'] = df_combined['GPP'].shift(1)\n",
    "    df_combined['lag_2'] = df_combined['GPP'].shift(2)\n",
    "    df_combined['rolling_mean'] = df_combined['GPP'].rolling(window=3).mean()\n",
    "\n",
    "    #Get time features\n",
    "    df_combined['hour'] = df_combined.index.hour\n",
    "    df_combined['day'] = df_combined.index.day\n",
    "    df_combined['month'] = df_combined.index.month\n",
    "    df_combined['year'] = df_combined.index.year\n",
    "\n",
    "    #Drop those rows where target variable contains NaN (can only drop when not intermediate NaNs - there are 8 sites with intermediate NaNs)\n",
    "    df_combined = df_combined.dropna(subset=['GPP'])\n",
    "    \n",
    "    #Drop first two rows and remove DateTimeIndex and reset it to integer-based index\n",
    "    df_combined = df_combined.iloc[2:].reset_index(drop=True)\n",
    "\n",
    "    #Drop categorical variable 'IGBP_veg_short' so that xgboost works properly\n",
    "    df_combined = df_combined.drop(columns=['IGBP_veg_short'])\n",
    "\n",
    "    #Consider dropping quality control variables\n",
    "    columns_to_drop = df_combined.filter(regex='_qc$').columns\n",
    "    df_clean = df_combined.drop(columns=columns_to_drop)\n",
    "\n",
    "    #Finally, scale data\n",
    "    scaled_data = scaler.fit_transform(df_clean)\n",
    "    df_final = pd.DataFrame(scaled_data, columns=df_clean.columns)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1defefc7-d8ae-49e5-bdbe-d7c5f9398c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack(df1, df2):\n",
    "    df_stacked = pd.concat([df1, df2], ignore_index=True)\n",
    "    return df_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddd66d80-e8d0-4b28-a9d0-39b7d4fdeeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize dataframe 1 & 2\n",
    "sites = list(filtered_files.keys())\n",
    "site_idx1 = 1\n",
    "df1 = initialize_dataframe(*filtered_files[sites[site_idx1]])\n",
    "\n",
    "sites = list(filtered_files.keys())\n",
    "site_idx2 = 2\n",
    "df2 = initialize_dataframe(*filtered_files[sites[site_idx2]])\n",
    "\n",
    "#Define X and y for each dataframe\n",
    "X1 = df1.drop(columns=['GPP'])\n",
    "y1 = df1['GPP']\n",
    "X2 = df2.drop(columns=['GPP'])\n",
    "y2 = df2['GPP']\n",
    "\n",
    "#Define train and test for each\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "#Stack for train and test\n",
    "X_train = stack(X1_train,X2_train)\n",
    "X_test = stack(X1_test,X2_test)\n",
    "y_train = stack(y1_train,y2_train)\n",
    "y_test = stack(y1_test,y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29a52939-344a-4ea4-9489-6701064aa16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.7548743636820404e-05, site: ('AT-Neu', 'AU-ASM')\n",
      "R² Score (in sample): 0.9983817982823652, site:('AT-Neu', 'AU-ASM')\n"
     ]
    }
   ],
   "source": [
    "model = XGBRegressor(learning_rate= .01, max_depth=10, n_estimators= 500)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "mse_site = mean_squared_error(y_test, predictions)\n",
    "#plt.plot(y_test)\n",
    "#plt.plot(predictions)\n",
    "print(f\"Mean Squared Error: {mse_site}, site: {sites[site_idx1],sites[site_idx2]}\")\n",
    "\n",
    "# Calculate R² Score\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² Score (in sample): {r2}, site:{sites[site_idx1],sites[site_idx2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38a8a861-5722-4f96-9894-ae95ed9c7fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize dataframe 1 & 2\n",
    "sites = list(filtered_files.keys())\n",
    "site_idx3 = 3\n",
    "df3 = initialize_dataframe(*filtered_files[sites[site_idx3]])\n",
    "\n",
    "#Define test feature and test target\n",
    "X_test = df3.drop(columns=['GPP'])\n",
    "y_test = df3['GPP']\n",
    "\n",
    "#Evaluate trained model on other site\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f84923c2-a355-48c7-94d9-b88cf98a326c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009106052555409302 -1.0851248685553645\n"
     ]
    }
   ],
   "source": [
    "print(mse,r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "370ca091-8a56-477b-a300-a4acce1350cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NEE': 77396.0, 'reco': 78076.0, 'Qh': 11184.0, 'Qle': 8324.0, 'Qg': 6532.0, 'rnet': 6615.0, 'CO2air': 7589.0, 'Tair': 9226.0, 'vpd': 5372.0, 'RH': 10714.0, 'Qair': 5254.0, 'Precip': 374.0, 'Psurf': 6774.0, 'SWdown': 4989.0, 'LWdown': 5360.0, 'Wind': 12691.0, 'SWdown_clearsky': 4209.0, 'LST_TERRA_Day': 6648.0, 'LST_TERRA_Night': 3720.0, 'EVI': 6717.0, 'NIRv': 2688.0, 'NDWI_band7': 4210.0, 'LAI': 2970.0, 'fPAR': 2497.0, 'lag_1': 25529.0, 'lag_2': 21572.0, 'rolling_mean': 23123.0, 'hour': 2744.0, 'day': 3750.0, 'month': 1641.0, 'year': 1063.0}\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the booster from the trained model\n",
    "booster = model.get_booster()\n",
    "\n",
    "# Get feature importance scores\n",
    "importance_scores = booster.get_score(importance_type='weight')  # Options: 'weight', 'gain', 'cover'\n",
    "\n",
    "print(importance_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c05c35-ea67-441b-884d-96454767564b",
   "metadata": {},
   "source": [
    "Now i want to stack some dataframes and train on several sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a61b078-3c77-4e41-856d-6a6aec08d7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/akmete/Setup/.venv/lib/python3.11/site-packages/sklearn/utils/_array_api.py:695: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/cluster/home/akmete/Setup/.venv/lib/python3.11/site-packages/sklearn/utils/_array_api.py:712: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            NEE      reco        Qh       Qle        Qg      rnet    CO2air  \\\n",
      "17252  0.412702  0.327183  0.307813  0.180921  0.392704       NaN  0.531377   \n",
      "2020   0.572029  0.336515  0.210938  0.177632  0.328326       NaN  0.069031   \n",
      "18697  0.653709  0.434201  0.085938  0.134868  0.236052  0.066734  0.515873   \n",
      "16889  0.622335  0.512018  0.103125  0.187500  0.268240  0.107179  0.526107   \n",
      "28672  0.570701  0.298334  0.110938  0.177632  0.358369  0.074823  0.818401   \n",
      "\n",
      "           Tair       vpd        RH  ...  fPAR  lag_1  lag_2  rolling_mean  \\\n",
      "17252  0.441375  0.117219  0.560185  ...   NaN    NaN    NaN           NaN   \n",
      "2020   0.794753  0.537539  0.154274  ...   NaN    NaN    NaN           NaN   \n",
      "18697  0.556768  0.303845  0.142493  ...   NaN    NaN    NaN           NaN   \n",
      "16889  0.787091  0.568161  0.088331  ...   NaN    NaN    NaN           NaN   \n",
      "28672  0.647783  0.391426  0.120739  ...   NaN    NaN    NaN           NaN   \n",
      "\n",
      "       hour  day  month  year       GPP  GPP  \n",
      "17252   NaN  NaN    NaN   NaN  0.586999  NaN  \n",
      "2020    NaN  NaN    NaN   NaN  0.435933  NaN  \n",
      "18697   NaN  NaN    NaN   NaN  0.382382  NaN  \n",
      "16889   NaN  NaN    NaN   NaN  0.429287  NaN  \n",
      "28672   NaN  NaN    NaN   NaN  0.428127  NaN  \n",
      "\n",
      "[5 rows x 92 columns]\n"
     ]
    }
   ],
   "source": [
    "#Create empty list\n",
    "sites = list(filtered_files.keys())\n",
    "dataframes = []\n",
    "sites_10 = sites[:10]\n",
    "\n",
    "#Loop through every site and add to dataframes list\n",
    "for site in sites_10:\n",
    "    df = initialize_dataframe(*filtered_files[site])\n",
    "    dataframes.append(df)\n",
    "\n",
    "for df in dataframes:\n",
    "    X = df.drop(columns=['GPP'])\n",
    "    y = df['GPP']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    df = pd.concat([X_train, X_test, y_train, y_test], axis=1)\n",
    "\n",
    "print(df.head())\n",
    "#Stack them now\n",
    "#df_stacked = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20217821-43c9-4633-84f2-c0b633e3353b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d92d2d-107e-4587-96b4-0104961ee760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1962e9-77ad-426e-ba34-18cfd37769d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
