{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd309de-ab7b-41bb-841b-7e7ee08fe2cb",
   "metadata": {},
   "source": [
    "### Leave-one-site out XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff7ec22a-a2f7-4967-b98f-25a4f47be0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2024/01/xgboost-for-time-series-forecasting/\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import netCDF4\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5b2f6c3-fce1-4441-bc21-1e14a7b81575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define path to data\n",
    "path = '/u/akmete/Documents/Master Thesis/Data/'\n",
    "files = [f for f in os.listdir(path) if f.endswith('.nc')] # all files\n",
    "files.sort()\n",
    "\n",
    "\n",
    "#Get the files out of folder which have intermediate NaN's. These are: DE-Lnf, IT-Ro2, SD-Dem, US-Los, US-Syv, US-WCr, US-Wi3 and ZM-Mon\n",
    "filt_list = ['DE-Lnf_flux.nc', 'DE-Lnf_meteo.nc', 'DE-Lnf_rs.nc', 'IT-Ro2_flux.nc', 'IT-Ro2_meteo.nc', 'IT-Ro2_rs.nc', 'SD-Dem_flux.nc', 'SD-Dem_meteo.nc', 'SD-Dem_rs.nc',\n",
    "'US-Los_flux.nc', 'US-Los_meteo.nc', 'US-Los_rs.nc', 'US-Syv_flux.nc', 'US-Syv_meteo.nc', 'US-Syv_rs.nc', 'US-WCr_flux.nc', 'US-WCr_meteo.nc', 'US-WCr_rs.nc',\n",
    "        'US-Wi3_flux.nc', 'US-Wi3_meteo.nc', 'US-Wi3_rs.nc', 'ZM-Mon_flux.nc', 'ZM-Mon_meteo.nc', 'ZM-Mon_rs.nc']\n",
    "\n",
    "# Remove files that are in list\n",
    "filtered_files = [file for file in files if file not in filt_list]\n",
    "assert len(filtered_files) % 3 == 0\n",
    "filtered_files = {filtered_files[0+3*i][:6]: (filtered_files[0+3*i],filtered_files[1+3*i],filtered_files[2+3*i]) for i in range(len(filtered_files)//3)}\n",
    "\n",
    "#Define list of sites\n",
    "sites = list(filtered_files.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7a16642-8271-4eb8-8319-d06ec5d1296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_good_quality(df):\n",
    "    for col in df.columns:\n",
    "        # Skip quality control columns\n",
    "        if col.endswith('_qc'):\n",
    "            continue\n",
    "        # Construct the corresponding quality control column name\n",
    "        qc_col = f\"{col}_qc\"\n",
    "        if qc_col in df.columns:\n",
    "            # Create a mask for bad quality data\n",
    "            bad_quality_mask = df[qc_col].isin([2, 3])\n",
    "            # Set bad quality data points to NaN\n",
    "            df.loc[bad_quality_mask, col] = np.nan\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "811b614c-89b1-41dd-bee2-dedac6c4393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dataframe(file1, file2, file3):\n",
    "    #Open data\n",
    "    ds = xr.open_dataset(path+file1, engine='netcdf4')\n",
    "    dr = xr.open_dataset(path+file2, engine='netcdf4')\n",
    "    dt = xr.open_dataset(path+file3, engine='netcdf4')\n",
    "\n",
    "    #Convert to dataframe\n",
    "    df = ds.to_dataframe()\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    df_meteo = dr.to_dataframe()\n",
    "    df_meteo = pd.DataFrame(df_meteo)\n",
    "\n",
    "    df_rs = dt.to_dataframe()\n",
    "    df_rs = pd.DataFrame(df_rs)\n",
    "\n",
    "    #Get rid of 'x' and 'y' in multiindex and get rid of 'latitude' and 'longitude' in meteo file\n",
    "    df = df.droplevel(['x','y'])\n",
    "    df_meteo = df_meteo.droplevel(['x','y'])\n",
    "    df_rs = df_rs.droplevel(['x','y'])\n",
    "    df = df.drop('latitude', axis=1)\n",
    "    df = df.drop('longitude', axis=1)\n",
    "    df_meteo = df_meteo.drop('latitude', axis=1)\n",
    "    df_meteo = df_meteo.drop('longitude', axis=1)\n",
    "\n",
    "    #Truncate rs so that same amount of rows as flux and meteo\n",
    "    df_rs = df_rs.truncate(after='2021-12-31 23:45:00')\n",
    "\n",
    "    #Merge dataframes\n",
    "    df_combined = pd.concat([df, df_meteo, df_rs],axis=1)\n",
    "\n",
    "    #Mark bad quality data (for all variables which allow for that)\n",
    "    df_combined = extract_good_quality(df_combined)\n",
    "\n",
    "    #Add lagged variables and rolling mean\n",
    "    df_combined['lag_1'] = df_combined['GPP'].shift(1)\n",
    "    df_combined['lag_2'] = df_combined['GPP'].shift(2)\n",
    "    df_combined['rolling_mean'] = df_combined['GPP'].rolling(window=3).mean()\n",
    "\n",
    "    #Get time features\n",
    "    df_combined['hour'] = df_combined.index.hour\n",
    "    df_combined['day'] = df_combined.index.day\n",
    "    df_combined['month'] = df_combined.index.month\n",
    "    df_combined['year'] = df_combined.index.year\n",
    "\n",
    "    #Drop those rows where target variable contains NaN (can only drop when not intermediate NaNs - there are 8 sites with intermediate NaNs)\n",
    "    df_combined = df_combined.dropna(subset=['GPP'])\n",
    "    \n",
    "    #Drop first two rows and remove DateTimeIndex and reset it to integer-based index\n",
    "    df_combined = df_combined.iloc[2:].reset_index(drop=True)\n",
    "\n",
    "    #Drop categorical variable 'IGBP_veg_short' so that xgboost works properly\n",
    "    df_combined = df_combined.drop(columns=['IGBP_veg_short'])\n",
    "\n",
    "    #Consider dropping quality control variables\n",
    "    columns_to_drop = df_combined.filter(regex='_qc$').columns\n",
    "    df_clean = df_combined.drop(columns=columns_to_drop)\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c7e3237-b9e0-4fd2-8e52-68a06aa266b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_scale(df):\n",
    "    \n",
    "    #Define train and test sets and convert them to numpy arrays\n",
    "    train_size = int(len(df) * 0.8)\n",
    "    train, test = df.iloc[:train_size], df.iloc[train_size:]\n",
    "\n",
    "    X_train = train.drop(columns=['GPP'])\n",
    "    y_train = train['GPP']\n",
    "    X_test = test.drop(columns=['GPP'])\n",
    "    y_test = test['GPP']\n",
    "\n",
    "    #Now scale everything and convert to dataframe then to numpy - scalind according to https://stackabuse.com/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python/\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    y_train = y_train.values.reshape(-1, 1)\n",
    "    y_test = y_test.values.reshape(-1, 1)\n",
    "\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "    y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "    return X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bef0e82d-a3b4-41b3-847a-da37fa7764d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(df):\n",
    "    # Features and target\n",
    "    X = df.drop(columns=[\"GPP\", \"latitude\", \"longitude\"])\n",
    "    y = df[\"GPP\"]\n",
    "\n",
    "    # Combine latitude and longitude to create a \"location group\"\n",
    "    df[\"location\"] = df[\"latitude\"].astype(str) + \"_\" + df[\"longitude\"].astype(str)\n",
    "    groups = df[\"location\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7718f6a-0784-468f-941c-4bf5c7a21956",
   "metadata": {},
   "source": [
    "Stack dataframes to one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8f8014b-649a-430c-9825-ea70a29a05d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m df_combined \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sites)):\n\u001b[0;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfiltered_files\u001b[49m\u001b[43m[\u001b[49m\u001b[43msites\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     df_combined \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_combined, df],ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[26], line 11\u001b[0m, in \u001b[0;36minitialize_dataframe\u001b[0;34m(file1, file2, file3)\u001b[0m\n\u001b[1;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mto_dataframe()\n\u001b[1;32m      9\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(df)\n\u001b[0;32m---> 11\u001b[0m df_meteo \u001b[38;5;241m=\u001b[39m \u001b[43mdr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m df_meteo \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(df_meteo)\n\u001b[1;32m     14\u001b[0m df_rs \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mto_dataframe()\n",
      "File \u001b[0;32m/usr/lib/python3.12/site-packages/xarray/core/dataset.py:7070\u001b[0m, in \u001b[0;36mDataset.to_dataframe\u001b[0;34m(self, dim_order)\u001b[0m\n\u001b[1;32m   7042\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert this dataset into a pandas.DataFrame.\u001b[39;00m\n\u001b[1;32m   7043\u001b[0m \n\u001b[1;32m   7044\u001b[0m \u001b[38;5;124;03mNon-index variables in this dataset form the columns of the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   7065\u001b[0m \n\u001b[1;32m   7066\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   7068\u001b[0m ordered_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_dim_order(dim_order\u001b[38;5;241m=\u001b[39mdim_order)\n\u001b[0;32m-> 7070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mordered_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mordered_dims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/site-packages/xarray/core/dataset.py:7039\u001b[0m, in \u001b[0;36mDataset._to_dataframe\u001b[0;34m(self, ordered_dims)\u001b[0m\n\u001b[1;32m   7034\u001b[0m data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   7035\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variables[k]\u001b[38;5;241m.\u001b[39mset_dims(ordered_dims)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   7036\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m columns\n\u001b[1;32m   7037\u001b[0m ]\n\u001b[1;32m   7038\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoords\u001b[38;5;241m.\u001b[39mto_index([\u001b[38;5;241m*\u001b[39mordered_dims])\n\u001b[0;32m-> 7039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/internals/construction.py:152\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    149\u001b[0m axes \u001b[38;5;241m=\u001b[39m [columns, index]\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/internals/managers.py:2139\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[0;34m(arrays, axes, consolidate, refs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_block_manager_from_column_arrays\u001b[39m(\n\u001b[1;32m   2122\u001b[0m     arrays: \u001b[38;5;28mlist\u001b[39m[ArrayLike],\n\u001b[1;32m   2123\u001b[0m     axes: \u001b[38;5;28mlist\u001b[39m[Index],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2135\u001b[0m     \u001b[38;5;66;03m# These last three are sufficient to allow us to safely pass\u001b[39;00m\n\u001b[1;32m   2136\u001b[0m     \u001b[38;5;66;03m#  verify_integrity=False below.\u001b[39;00m\n\u001b[1;32m   2138\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2139\u001b[0m         blocks \u001b[38;5;241m=\u001b[39m \u001b[43m_form_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2140\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m BlockManager(blocks, axes, verify_integrity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   2141\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/internals/managers.py:2212\u001b[0m, in \u001b[0;36m_form_blocks\u001b[0;34m(arrays, consolidate, refs)\u001b[0m\n\u001b[1;32m   2209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(dtype\u001b[38;5;241m.\u001b[39mtype, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[1;32m   2210\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m-> 2212\u001b[0m values, placement \u001b[38;5;241m=\u001b[39m \u001b[43m_stack_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtup_block\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dtlike:\n\u001b[1;32m   2214\u001b[0m     values \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(values)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/internals/managers.py:2252\u001b[0m, in \u001b[0;36m_stack_arrays\u001b[0;34m(tuples, dtype)\u001b[0m\n\u001b[1;32m   2249\u001b[0m first \u001b[38;5;241m=\u001b[39m arrays[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2250\u001b[0m shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(arrays),) \u001b[38;5;241m+\u001b[39m first\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m-> 2252\u001b[0m stacked \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, arr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n\u001b[1;32m   2254\u001b[0m     stacked[i] \u001b[38;5;241m=\u001b[39m arr\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_combined = pd.DataFrame()\n",
    "for i in range(len(sites)):\n",
    "    df = initialize_dataframe(*filtered_files[sites[i]])\n",
    "    df_combined = pd.concat([df_combined, df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8b75ca-ec7a-44fa-bb81-844d8c140d08",
   "metadata": {},
   "source": [
    "Define Feature X and target y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a08dabe-a7de-4f18-8d82-bd29306a37ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_combined.drop(columns=[\"GPP\", \"latitude\", \"longitude\"])\n",
    "y = df_combined[\"GPP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c11be04-c60b-461b-82ba-d45fb7eccc0a",
   "metadata": {},
   "source": [
    "Define new feature \"location\" for the Leave-One-Out CV later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d43e63c-2b78-4521-bb1e-dd14c429905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"location\"] = (df_combined[\"latitude\"].round(4).astype(str) + \"_\" + df_combined[\"longitude\"].round(4).astype(str))\n",
    "groups = df_combined[\"location\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db323c-1601-4a13-b49a-7a4b73cf6ea7",
   "metadata": {},
   "source": [
    "Initialize LOGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02080dcc-3e3f-4ee4-b633-e2ef9885bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "logo = LeaveOneGroupOut()\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9811b752-58e0-418f-9748-1be0e94d5036",
   "metadata": {},
   "source": [
    "Go through each location split the data, apply minmax scaler and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb3b37-b018-4e5a-a803-b3445f0b6152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each group (location) in the cross-validation\n",
    "for train_idx, test_idx in logo.split(X, y, groups=groups):\n",
    "    # Split the data\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Step 4: Define the pipeline with MinMaxScaler and XGBoost\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),  # MinMax scaling\n",
    "        ('xgb', XGBRegressor())     # XGBoost model\n",
    "    ])\n",
    "\n",
    "    # Train the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Calculate performance metric\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    test_location = groups.iloc[test_idx].iloc[0]  # The left-out location\n",
    "    results.append({\"location\": test_location, \"mse\": mse})\n",
    "\n",
    "# Step 5: Results as a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display results\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
